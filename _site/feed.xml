<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Mediumish</title>
    <description>An overview of some of the things i've done!</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Mon, 01 Dec 2025 00:51:28 -0500</pubDate>
    <lastBuildDate>Mon, 01 Dec 2025 00:51:28 -0500</lastBuildDate>
    <generator>Jekyll v3.8.5</generator>
    
      <item>
        <title>Game Storyizer</title>
        <description>&lt;p&gt;I’ve built a new tool called &lt;strong&gt;Game Storyizer&lt;/strong&gt;. It’s a node-based narrative builder designed to help game developers and writers create complex, branching storylines.&lt;/p&gt;

&lt;p&gt;The tool features a visual graph editor where you can create nodes representing story events and connect them to define the flow. It supports logic gates (AND/OR) for prerequisites, making it easy to design puzzles or multi-path narratives.&lt;/p&gt;

&lt;p&gt;I wanted to try to see how good Gemini Canvas Mode was at building games, but i also needed a way to present the game story to the AI. Just writing it out in plain text wasn’t good enough because stories have branches, and if you need to change or modify things, that gets complicated.&lt;/p&gt;

&lt;p&gt;The idea of using a directed graph story editor isn’t new, but i couldn’t find any free easy to use tools, that most importantly, just outputted the story in a format good for AI. This tool is the beginnings of that. I already have some plans to make this better. I belief one-shot creation of a game from story to 3D world is entirely possible!&lt;/p&gt;

&lt;p&gt;Check it out below!&lt;/p&gt;

&lt;div style=&quot;margin-bottom: 10px; text-align: right;&quot;&gt;
&lt;a href=&quot;/assets/apps/game-storyizer/dist/index.html&quot; target=&quot;_blank&quot; style=&quot;display: inline-block; padding: 8px 16px; background-color: #4f46e5; color: white; text-decoration: none; border-radius: 4px; font-weight: bold; font-size: 12px;&quot;&gt;OPEN IN NEW WINDOW&lt;/a&gt;
&lt;/div&gt;

&lt;iframe src=&quot;/assets/apps/game-storyizer/dist/index.html&quot; style=&quot;width:100%; height:800px; border:none; background: #020617;&quot;&gt;&lt;/iframe&gt;
</description>
        <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/game-storyizer/</link>
        <guid isPermaLink="true">http://localhost:4000/game-storyizer/</guid>
        
        
        <category>gamedev</category>
        
        <category>writing</category>
        
        <category>tool</category>
        
      </item>
    
      <item>
        <title>Find Your Jacket: A Winter Puzzle</title>
        <description>&lt;p&gt;I’m excited to share a new mini-game: &lt;strong&gt;Find Your Jacket&lt;/strong&gt;. It’s a short, atmospheric puzzle game set in a frozen cabin.&lt;/p&gt;

&lt;p&gt;This game was actually built using my new narrative tool, &lt;a href=&quot;/game-storyizer/&quot;&gt;Game Storyizer&lt;/a&gt;. I used the tool to map out the puzzle logic and item interactions, then exported the data to drive the game’s state machine. It was a great way to test the tool in a real-world scenario!&lt;/p&gt;

&lt;p&gt;Most amazingly, i built this all on my phone, lounging on my couch, just talking to an AI. In my 35 years of programming, writing code since i was in 4th grade, this was the most blissful programming experience i’ve ever had. We’re truly in a new era of AI driven creativity!!&lt;/p&gt;

&lt;p&gt;Explore the cabin, find items, and see if you can escape the cold. It works great on mobile. You can use WASD keys on desktop, and click and drag to move the camera and interact.&lt;/p&gt;

&lt;div style=&quot;margin-bottom: 10px; text-align: right;&quot;&gt;
&lt;a href=&quot;/assets/apps/game-puzzle-jacket/index.html&quot; target=&quot;_blank&quot; style=&quot;display: inline-block; padding: 8px 16px; background-color: #4f46e5; color: white; text-decoration: none; border-radius: 4px; font-weight: bold; font-size: 12px;&quot;&gt;OPEN IN NEW WINDOW&lt;/a&gt;
&lt;/div&gt;

&lt;iframe src=&quot;/assets/apps/game-puzzle-jacket/index.html&quot; style=&quot;width:100%; height:800px; border:none; background: #000;&quot;&gt;&lt;/iframe&gt;
</description>
        <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/find-your-jacket/</link>
        <guid isPermaLink="true">http://localhost:4000/find-your-jacket/</guid>
        
        
        <category>gamedev</category>
        
        <category>puzzle</category>
        
        <category>webgl</category>
        
      </item>
    
      <item>
        <title>MIDI Poly-Sequencer Synth</title>
        <description>&lt;p&gt;Inspired by instagram videos of a user playing a “stylophone” i wanted to see if i could replicate this on an iPhone. There are many tools that could possibly be used to do this, but i decided to try out Google Gemini’s mobile iOS app. It has a “Canvas” mode where the AI will generate react-based apps and other types of content, that you an view directly in your browser. The results were incredible. The AI did exactly what i asked, and even when I thought it wouldn’t be able to parse a MIDI file, it did it without any issues.&lt;/p&gt;

&lt;p&gt;I continued to use the AI to iterate the app, all while lounging on the couch, and because of the features of modern day browser, the app runs great and feels like a native app. It scratched the musical itch i was having.&lt;/p&gt;

&lt;p&gt;Check it out below!&lt;/p&gt;

&lt;div style=&quot;margin-bottom: 10px; text-align: right;&quot;&gt;
&lt;a href=&quot;/assets/apps/midi-sequencer/index.html&quot; target=&quot;_blank&quot; style=&quot;display: inline-block; padding: 8px 16px; background-color: #2563eb; color: white; text-decoration: none; border-radius: 4px; font-weight: bold; font-size: 12px;&quot;&gt;OPEN IN NEW WINDOW&lt;/a&gt;
&lt;/div&gt;

&lt;iframe src=&quot;/assets/apps/midi-sequencer/index.html&quot; style=&quot;width:100%; height:800px; border:none; background: #000;&quot;&gt;&lt;/iframe&gt;

&lt;h3 id=&quot;features&quot;&gt;Features&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Auto-Scroll&lt;/strong&gt;: The sequencer automatically scrolls to follow the playback.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;MIDI Import&lt;/strong&gt;: Import your own MIDI files to visualize and play them.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Touch Support&lt;/strong&gt;: Includes a touch pad for interactive play.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Synthesizer&lt;/strong&gt;: Built-in oscillator-based synthesizer.&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Sun, 30 Nov 2025 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/midi-sequencer/</link>
        <guid isPermaLink="true">http://localhost:4000/midi-sequencer/</guid>
        
        
        <category>hobby</category>
        
        <category>music</category>
        
        <category>react</category>
        
        <category>interactive</category>
        
      </item>
    
      <item>
        <title>Cerebras OS - OpenRouter Hackathon Project</title>
        <description>&lt;p&gt;When AI becomes fast enough, computing itself changes. Cerebras OS is a hackathon project exploring what happens when inference speed crosses a threshold—when AI stops being something you wait for and becomes something you interact with.&lt;/p&gt;

&lt;h2 id=&quot;a-new-threshold&quot;&gt;A New Threshold&lt;/h2&gt;

&lt;p&gt;&lt;iframe style=&quot;width:100%;&quot; height=&quot;515&quot; src=&quot;https://www.youtube.com/embed/JLiflqAjGQg&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;

&lt;p&gt;We’re used to designing around AI latency. You build your UI, add a loading spinner, send a request to the AI, and eventually get back a response. This pattern has shaped how we think about AI-powered applications: the AI is always somewhere else, always asynchronous, always a step removed from the interaction.&lt;/p&gt;

&lt;p&gt;But what if it wasn’t? What if AI inference was so fast that it could live &lt;em&gt;inside&lt;/em&gt; the interaction loop itself?&lt;/p&gt;

&lt;p&gt;That’s the question I set out to explore with Cerebras OS. Using Cerebras’ ultra-fast inference through OpenRouter, I built an interface where every interaction—every click, every widget request, every UI update—flows through an LLM in real-time. Not as a background process. Not with loading states. Just immediate, responsive, and always there.&lt;/p&gt;

&lt;h2 id=&quot;the-demo-widgets-as-a-proof-of-concept&quot;&gt;The Demo: Widgets as a Proof of Concept&lt;/h2&gt;

&lt;p&gt;The interface I built uses widgets—small, dynamic components that can display anything from weather to task lists. But the widgets themselves aren’t the point. They’re just a way to demonstrate something more fundamental: when AI is fast enough, you can stop pre-programming your interface and start &lt;em&gt;generating&lt;/em&gt; it on demand.&lt;/p&gt;

&lt;p&gt;Ask for a weather widget, and the AI creates one instantly. It generates the HTML, decides on the styling, pulls in live data, and updates it in real-time. Want something different? Just ask. The AI adapts, evolves, and responds without you ever feeling like you’re waiting for a machine to think.&lt;/p&gt;

&lt;p&gt;This isn’t about making better widgets. It’s about what becomes possible when you can put an LLM directly in the decision-making path of your UI. Every interaction becomes an opportunity for the software to be smarter, more contextual, more adaptive—not because you pre-programmed those paths, but because the AI can make those decisions in the moment.&lt;/p&gt;

&lt;h2 id=&quot;why-speed-matters-a-paradigm-shift&quot;&gt;Why Speed Matters: A Paradigm Shift&lt;/h2&gt;

&lt;p&gt;We spend a lot of time talking about AI capabilities—what models can do, what tasks they can solve, what benchmarks they can pass. But speed is different. Speed isn’t just about doing the same things faster; it unlocks entirely new ways of building software.&lt;/p&gt;

&lt;p&gt;When Cerebras can return responses in under a second, consistently, something shifts. The AI stops being a tool you call and becomes something more fundamental—a layer of intelligence woven into the fabric of your application. You stop designing around it and start designing &lt;em&gt;with&lt;/em&gt; it.&lt;/p&gt;

&lt;p&gt;In traditional software, you anticipate user needs and pre-build flows for every scenario. With fast AI, you can generate those flows on demand. The interface becomes fluid, adaptive, personal—not because you wrote a thousand if-statements, but because the AI can understand context and respond in real-time.&lt;/p&gt;

&lt;p&gt;This is the real promise of Cerebras OS, and projects like it. They’re not about widgets or chat interfaces or any specific feature. They’re about exploring what computing looks like when intelligence is synchronous with interaction.&lt;/p&gt;

&lt;h2 id=&quot;building-it&quot;&gt;Building It&lt;/h2&gt;

&lt;p&gt;I built this using Blazor Server and .NET 8 for real-time WebSocket connections, with qwen3-32b running on Cerebras hardware through OpenRouter. The model was chosen for its speed and ability to output structured JSON—critical for generating consistent, usable HTML.&lt;/p&gt;

&lt;p&gt;The architecture is deliberately simple: route everything through the AI. In most systems, this would be a terrible idea. But when your inference is fast enough, simplicity becomes viable. You don’t need complex caching strategies or pre-computation. You just ask the AI what to do next.&lt;/p&gt;

&lt;p&gt;There’s also a background processing system that pulls in context from the web using OpenRouter’s search capabilities, enriching the AI’s responses without impacting the interaction speed. But the core insight remains: when AI is fast, your architecture can be fundamentally different.&lt;/p&gt;

&lt;h2 id=&quot;the-bigger-picture&quot;&gt;The Bigger Picture&lt;/h2&gt;

&lt;p&gt;This project is aspirational. It’s not a production system; it’s a sketch of what could be. Just like early flight demonstrations weren’t about the specific aircraft but about proving that flight was possible, Cerebras OS is about proving that synchronous AI-powered interfaces are possible.&lt;/p&gt;

&lt;p&gt;We’re at the beginning of a shift. As inference speeds continue to improve, we’ll see new categories of software that simply couldn’t exist before:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Interfaces that adapt in real-time to user intent without explicit programming&lt;/li&gt;
  &lt;li&gt;Applications that generate their own UI based on context and conversation&lt;/li&gt;
  &lt;li&gt;Systems where the boundary between “the application” and “the AI” dissolves entirely&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The widgets in this demo are just placeholders. Tomorrow it might be entire application flows generated on demand. The day after, interfaces that evolve as you use them. The underlying principle remains: when AI is fast enough to be synchronous, computing enters a new paradigm.&lt;/p&gt;

&lt;p&gt;We’re still years away from seeing this widely deployed. Infrastructure needs to be built. Patterns need to be discovered. Costs need to come down. But the threshold has been crossed. Fast AI isn’t just faster AI—it’s a different kind of tool entirely.&lt;/p&gt;

&lt;h2 id=&quot;try-it-yourself&quot;&gt;Try It Yourself&lt;/h2&gt;

&lt;p&gt;The code is on GitHub if you want to explore. You’ll need an OpenRouter API key:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;OPENROUTER_API_KEY&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;your_key_here
dotnet run
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Play with it. Break it. Imagine what else could be built if AI was always this fast. That’s the real point of projects like this—not what they are, but what they help us imagine.&lt;/p&gt;
</description>
        <pubDate>Sat, 24 May 2025 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/cerebrasos/</link>
        <guid isPermaLink="true">http://localhost:4000/cerebrasos/</guid>
        
        
        <category>hobby</category>
        
        <category>hackathon</category>
        
        <category>AI</category>
        
        <category>youtube</category>
        
        <category>github</category>
        
      </item>
    
      <item>
        <title>Apple Intelligence features for any iPhone</title>
        <description>&lt;p&gt;Apple prides itself on safety and privacy, but if you’re willing to sacrifice some of that, and you have an older iPhone, you can use the Shortcuts app to create a shortcut that will use OpenAI’s API to generate text in situ.&lt;/p&gt;

&lt;p&gt;In this post:&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#using-ai-to-modify-text-in-any-text-field&quot; id=&quot;markdown-toc-using-ai-to-modify-text-in-any-text-field&quot;&gt;Using AI to modify text in any text field&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#using-ai-to-respond-to-an-imessage&quot; id=&quot;markdown-toc-using-ai-to-respond-to-an-imessage&quot;&gt;Using AI to respond to an iMessage&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#download-the-shortcuts&quot; id=&quot;markdown-toc-download-the-shortcuts&quot;&gt;Download the shortcuts&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;using-ai-to-modify-text-in-any-text-field&quot;&gt;Using AI to modify text in any text field&lt;/h2&gt;

&lt;p&gt;Apple Shortcuts is an app that lets you combine system level automation with the apps on your iPhone. The app developer specifically has to create hooks for this (if you’ve ever used AppleScript you have seen how this works), and OpenAI has done a decent job of creating a hooks for their iOS app (as of this writing Google’s Gemini App does not support Shortcuts).&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Download the ChatGPT app from the App Store and sign in with your OpenAI account&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Create a new Shortcut in the Shortcuts app that looks like this:
&lt;img src=&quot;/assets/images/ShortcutsSettings1.png&quot; alt=&quot;Shortcut Setup&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Open any app with a text input field (Messages, Reddit, Notes, etc.)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Highlight some text and tap the Share button, then select your new Shortcut from the share sheet to generate AI-enhanced text right in the app. Once you click “Done” the generated text is copied to your clipboard, you can paste it and modify it if you want
&lt;img src=&quot;/assets/images/ShortcutsUse1.png&quot; alt=&quot;Asking ChatGPT&quot; /&gt;
&lt;em&gt;Note: You should have recently launched the ChatGPT app so your account is logged in&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;using-ai-to-respond-to-an-imessage&quot;&gt;Using AI to respond to an iMessage&lt;/h2&gt;
&lt;p&gt;There’s a couple of methods out there to do this. One method keeps a duplicate of all recieved messages in an iOS Note, and uses this as the context for the AI to generate new messages. You can seek this out online if you’re interested. The method i detail here lets you take a screeenshot of an iOS conversation, it uses the iOS System OCR to extract all the text, it then crops out the right and left half of the screenshot and parses the text. Then finally it sends these parsed pieces of text to ChatGPT to interpret who said what, and generate a response.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Download the shortcut linked in this post. This shortcut is too long to share in a screenshot. Shortcuts are usually safe to install and you can see the code in the Shortcuts app for yourself to verify nothing malicious is happening&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Open a Messages conversation&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Take a screenshot of the conversation and select the preview iOS pops up&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Choose “Share” and wait for the response from the AI.
&lt;em&gt;Note: You should have recently launched the ChatGPT app so your account is logged in&lt;/em&gt;
&lt;img src=&quot;/assets/images/ShortcutsUse2s1.png&quot; alt=&quot;Shortcut Usage Messages&quot; /&gt;
&lt;img src=&quot;/assets/images/ShortcutsUse2s2.png&quot; alt=&quot;Shortcut Usage Messages2&quot; /&gt;
&lt;em&gt;Make sure to delete the screenshot and don’t “Copy” since the AI response will be in the clipboard&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Paste the AI response into the conversation and send it&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;download-the-shortcuts&quot;&gt;Download the shortcuts&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.icloud.com/shortcuts/09e5549a74a7490993647d58a3cbc72c&quot;&gt;AI text style and modifier&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.icloud.com/shortcuts/17899ad80a1d467ea269f9365d06de58&quot;&gt;AI Messages responder&lt;/a&gt;&lt;/p&gt;

</description>
        <pubDate>Sun, 15 Dec 2024 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/AIopenai/</link>
        <guid isPermaLink="true">http://localhost:4000/AIopenai/</guid>
        
        
        <category>hobby</category>
        
        <category>AI</category>
        
        <category>iOS</category>
        
        <category>tutorial</category>
        
      </item>
    
      <item>
        <title>Vidable.ai</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://www.vidable.ai/&quot;&gt;Vidable.ai&lt;/a&gt; was a startup I worked for. It was a platform that connected to video sources, and later websites and documents, to feed an AI system. The initial release focused on a chat assistant feature, but the main R&amp;amp;D focus i lead was looking beyond the chat box, and into creating ai generated “artifacts”. Once AI has access to a customer’s specialized data, the thinking was that by the selective nature of this data, the AI could autonomously understand the market niche, and based on the content, determine on its own what sort of things to create, from documents, to applets, to push communications.&lt;/p&gt;
</description>
        <pubDate>Wed, 01 May 2024 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/vidable/</link>
        <guid isPermaLink="true">http://localhost:4000/vidable/</guid>
        
        
        <category>profession</category>
        
        <category>AI</category>
        
        <category>terraform</category>
        
        <category>devops</category>
        
        <category>microservices</category>
        
        <category>docker</category>
        
      </item>
    
      <item>
        <title>Apple Vision Pro Music Viz App</title>
        <description>&lt;p&gt;Currently a WIP, but using audio processing APIs and particle emitter SDK, i’m working on a music visualizer app to be completely immersed in a 3D field of dancing particles.&lt;/p&gt;
</description>
        <pubDate>Fri, 01 Mar 2024 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/mvizapp/</link>
        <guid isPermaLink="true">http://localhost:4000/mvizapp/</guid>
        
        
        <category>hobby</category>
        
        <category>music</category>
        
        <category>swift</category>
        
        <category>xcode</category>
        
        <category>visionpro</category>
        
      </item>
    
      <item>
        <title>Why you may not have to worry about superintelligent AI as much: Entropy</title>
        <description>&lt;p&gt;&lt;strong&gt;The Simple Reason You Don’t Have to Worry About Super Intelligent AI: Entropy&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The advent of AI superintelligence is imminent, likely within the next decade. This rapid progression toward advanced AI has sparked widespread concern about the potential consequences of such powerful technology. The crux of the matter lies in the alignment problem: how can we ensure that AI behaves in ways that are beneficial to humanity? The simple truth is, we can’t implicitly align AI with human values. Good people will create good AI, and evil people will create evil AI. This age-old struggle between good and evil will inevitably play out in the realm of artificial intelligence.&lt;/p&gt;

&lt;p&gt;Our best hope lies in the creation of more good AI than evil. The proliferation of benevolent AI systems, designed and operated by individuals and organizations with ethical intentions, can help counterbalance the malevolent uses of AI. However, even if we fail to achieve this balance, there’s a fundamental principle that provides a silver lining: entropy.&lt;/p&gt;

&lt;p&gt;Entropy, a concept rooted in thermodynamics and information theory, dictates that in any system, disorder tends to increase over time. This principle applies to AI systems as well. No matter how advanced or powerful an AI becomes, it will face inherent limitations. Even with infinite computational power and memory, an AI cannot simulate an open system faster than the system runs itself. To make predictions, AI must rely on heuristics, which inevitably introduce errors.&lt;/p&gt;

&lt;p&gt;As time progresses, these errors accumulate. Predictions made by even the most advanced AI will, after some number of iterations, begin to resemble random noise. This inherent uncertainty means that no AI, regardless of its computational prowess, can maintain perfect accuracy indefinitely. Eventually, all predictions will degrade into chaos.&lt;/p&gt;

&lt;p&gt;Yet, within this seemingly chaotic landscape, one prediction will still be right. This randomness levels the playing field, allowing even a lower-compute rival to potentially best an infinite-compute adversary through sheer luck or superior observation of the system’s state. This dynamic ensures that the world reverts to the familiar human battles we have always fought and won.&lt;/p&gt;

&lt;p&gt;The concept of entropy assures us that the future of AI will not be dominated by a single, all-powerful entity. Instead, it will be a landscape of competing intelligences, each with its own strengths and weaknesses. This inherent unpredictability preserves the opportunity for human ingenuity and resilience to prevail.&lt;/p&gt;

&lt;p&gt;While the rise of AI superintelligence may seem daunting, the principles of entropy should provide a somewhat comforting perspective. The inevitable accumulation of errors in AI predictions ensures that no single intelligence can maintain dominance indefinitely. This inherent uncertainty offers hope that the age-old human struggle between good and evil will continue, and with it, the possibility for good to triumph. As we navigate this brave new world, our focus should be on fostering ethical AI development and leveraging the surprises of entropy to keep the scales balanced.&lt;/p&gt;
</description>
        <pubDate>Wed, 01 Nov 2023 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/aisafety/</link>
        <guid isPermaLink="true">http://localhost:4000/aisafety/</guid>
        
        
        <category>hobby</category>
        
        <category>writing</category>
        
        <category>alignment</category>
        
        <category>research</category>
        
        <category>AI</category>
        
      </item>
    
      <item>
        <title>LLM Visualization: How embedding space creates intelligence</title>
        <description>&lt;p&gt;Using Python I created this visualization to help explain how LLMs capture and synthesize information. LLMs take the encoding of language and deconstruct it into concepts.&lt;/p&gt;

&lt;p&gt;Multimodal LLMs take sound/images and map them to the concept spaces.&lt;/p&gt;

&lt;p&gt;Then when you prompt a model with a question, the prompt establishes a “road” in a region of space that maps to related concepts, and the AI just continues on the most sensible path from this road to respond to the prompt.&lt;/p&gt;

&lt;p&gt;The road, rather than being in 3 dimensions, is in thousands of dimensions, and this is what makes AI seem intelligent. The road can also be very winding and serpentine, it doesn’t have to be a straight line, and rarely is when you consider that an LLM can have 3 thousand dimensions or more that it’s carving a path through.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/llmpath.png&quot; alt=&quot;A smooth path vs a spiraly winding path&quot; /&gt;
&lt;em&gt;Image Caption: A comparison of a smooth path and a spirally winding path.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;For some additional reading on the topic, I recommend the following articles:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://moebio.com/mind/&quot;&gt;Moebio&lt;/a&gt; live interactive visualization of a latent space and how it connects to a text prompt. By far the best visualization I’ve seen of this concept. (note page may take a minute or so to load)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.rungalileo.io/galileo/how-to-and-faq/galileo-product-features/embeddings-view&quot;&gt;Galileo&lt;/a&gt; This is an embedding space viewer and clustering tool that can be used to visualize the concept space of an LLM&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/enjalot/latent-scope&quot;&gt;LatentScope&lt;/a&gt; Another tool for visualizing an embedding space&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note that any tool to turn a thousand dimensional space to 2D or 3D will always be a lossy representation&lt;/p&gt;
</description>
        <pubDate>Sun, 17 Sep 2023 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/llmviz/</link>
        <guid isPermaLink="true">http://localhost:4000/llmviz/</guid>
        
        
        <category>hobby</category>
        
        <category>research</category>
        
        <category>python</category>
        
        <category>AI</category>
        
        <category>visualization</category>
        
      </item>
    
      <item>
        <title>Where the puck is going: An AI Roadmap</title>
        <description>&lt;p&gt;I created this AI roadmap last year for our internal team, and we’re somewhere around line 6.5 as of May 2024 (current video multimodal models are just looking at periodic frames so I don’t count that as true video yet) and on the lower bound of my time estimates. Every step along the way though will have a huge rush of innovation and development and experimentation of new product categories, which is very exciting, but can also sometimes create red herrings that people chase after.&lt;/p&gt;

&lt;p&gt;It’s important to keep the end goal in mind when doing product development so you don’t stagnate in a local maximum.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/airoadmap.png&quot; alt=&quot;A tech tree for a video library app&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 25 Aug 2023 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/airoadmap/</link>
        <guid isPermaLink="true">http://localhost:4000/airoadmap/</guid>
        
        
        <category>profession</category>
        
        <category>research</category>
        
        <category>AI</category>
        
      </item>
    
  </channel>
</rss>
