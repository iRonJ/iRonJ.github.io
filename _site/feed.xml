<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Mediumish</title>
    <description>An overview of some of the things i've done!</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sat, 18 May 2024 15:56:05 -0400</pubDate>
    <lastBuildDate>Sat, 18 May 2024 15:56:05 -0400</lastBuildDate>
    <generator>Jekyll v3.8.5</generator>
    
      <item>
        <title>Vidable.ai</title>
        <description>&lt;p&gt;Vidable.ai was a project I worked on during my time at &lt;a href=&quot;https://www.vidable.ai/&quot;&gt;Vidable&lt;/a&gt;. It was a platform that was designed to connect to video sources, and later websites and documents, to feed an AI system. The initial release focused on a chat assistant feature, but the main R&amp;amp;D focus i lead was looking beyond the chat box, and into creating ai generated “artifacts”. Once AI has access to a customer’s specialized data, the thinking was that by the selective nature of this data, the AI could autonomously understand the market niche, and based on the content, determine on its own what sort of things to create, from documents, to applets, to push communications.&lt;/p&gt;
</description>
        <pubDate>Wed, 01 May 2024 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/vidable/</link>
        <guid isPermaLink="true">http://localhost:4000/vidable/</guid>
        
        
        <category>profession</category>
        
        <category>AI</category>
        
      </item>
    
      <item>
        <title>Apple Vision Pro Music Viz App</title>
        <description>&lt;p&gt;Currently a WIP, but using audio processing APIs and particle emitter SDK, i’m working on a music visualizer app to be completely immersed in a 3D field of dancing particles.&lt;/p&gt;
</description>
        <pubDate>Fri, 01 Mar 2024 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/mvizapp/</link>
        <guid isPermaLink="true">http://localhost:4000/mvizapp/</guid>
        
        
        <category>hobby</category>
        
        <category>music</category>
        
        <category>swift</category>
        
        <category>xcode</category>
        
        <category>visionpro</category>
        
      </item>
    
      <item>
        <title>LessWrong: ChatGPT is our Wright Brothers Moment</title>
        <description>&lt;p&gt;There’s a lot of discussions, often derisively, that many AI apps are “just” ChatGPT wrappers. I wrote a short article that was promoted to the &lt;a href=&quot;https://www.lesswrong.com/posts/3TjfowjTcHEL56Nyp/chatgpt-is-our-wright-brothers-moment&quot;&gt;front page of LessWrong&lt;/a&gt; about this, trying to convey that just because a tool is an AI wrapper doesn’t mean it’s not significant.&lt;/p&gt;

&lt;p&gt;AI is opening up a whole new spectrum of possibile experiences, but just like how it took decades for air travel to be commcercialized due to a huge amount of infrastructure needing to be built, we are years away from seeing the impact of AI on our daily lives due to a multitude of new software infrastructure needing to be built.&lt;/p&gt;
</description>
        <pubDate>Sat, 24 Dec 2022 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/wrightbrochatgpt/</link>
        <guid isPermaLink="true">http://localhost:4000/wrightbrochatgpt/</guid>
        
        
        <category>hobby</category>
        
        <category>AI</category>
        
        <category>writing</category>
        
      </item>
    
      <item>
        <title>Cohere AI: LLM Hackathon #3 3rd place winner</title>
        <description>&lt;p&gt;I participated practically solo in the CoHere AI Hackathon to use an early version of the Cohere LLM to make an app. The project came in third out of thousands of participants and dozens of valid submissions. The project used real-time live voice transcription that was entirely browser based, HTML Canvas compositing for live webcam video input, and a fully custom RAG (retrieval augmented generation) code written in C# before RAG was even a known terminology. Information was retrieved from Wikipedia to help ground the AI’s responses.&lt;/p&gt;

&lt;p&gt;&lt;iframe style=&quot;width:100%;&quot; height=&quot;515&quot; src=&quot;https://www.youtube.com/embed/8DcO6GYUF_0?si=T1IR99vX1GIRZZ0i&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 12 Nov 2022 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/hackathon/</link>
        <guid isPermaLink="true">http://localhost:4000/hackathon/</guid>
        
        
        <category>hobby</category>
        
        <category>hackathon</category>
        
        <category>AI</category>
        
      </item>
    
      <item>
        <title>EleutherAI: Tool Use Idea</title>
        <description>&lt;p&gt;I created a &lt;a href=&quot;https://github.com/EleutherAI/project-menu/issues/22#issuecomment-1073167091&quot;&gt;working diagram&lt;/a&gt; of how a set of agent AI models could be used to answer math questions using tools. At the time I wrote this post, tool use wasn’t commonly understood, nor was agent AI based processes.&lt;/p&gt;

&lt;p&gt;There are lots of powerful things you can do with the Markdown editor. If you’ve gotten pretty comfortable with writing in Markdown, then you may enjoy some more advanced tips about the types of things you can do with Markdown!&lt;/p&gt;

&lt;p&gt;As with the last post about the editor, you’ll want to be actually editing this post as you read it so that you can see all the Markdown code we’re using.&lt;/p&gt;

</description>
        <pubDate>Sun, 20 Mar 2022 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/eleuther/</link>
        <guid isPermaLink="true">http://localhost:4000/eleuther/</guid>
        
        
        <category>AI</category>
        
        <category>hobby</category>
        
        <category>research</category>
        
      </item>
    
      <item>
        <title>SwiftUI Mars App</title>
        <description>&lt;p&gt;As an aficianado of the Mars Rover missions, I decided to create a simple app that would allow me to view the latest images from the Mars Rover missions. I used SwiftUI and Xcode to create the app, and I’m quite happy with the results. The app is simple, but it does what I want it to do: show me the latest images from Mars in an easy to use UI.&lt;/p&gt;

&lt;p&gt;Dare mighty things!&lt;/p&gt;
</description>
        <pubDate>Mon, 08 Mar 2021 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/marsapp/</link>
        <guid isPermaLink="true">http://localhost:4000/marsapp/</guid>
        
        
        <category>hobby</category>
        
        <category>mars</category>
        
        <category>swift</category>
        
        <category>xcode</category>
        
      </item>
    
      <item>
        <title>Empowering blind instructors to control classroom technology</title>
        <description>&lt;p&gt;One of my more successful early projects was creating a way to &lt;a href=&quot;https://www.youtube.com/watch?v=thNedzQu1MA&quot;&gt;allow blind users to control classroom&lt;/a&gt; technology. The implementation used features of Crestron panels in a clever way to allow users, both blind and sighted, to swipe through a “virtual list” that could be built from just a text configuration. The gesture approach mirrored Apple’s VoiceOver screenreader, although I was completely unaware of VoiceOver at the time– the similarity was purely coincidental.&lt;/p&gt;
</description>
        <pubDate>Thu, 14 Feb 2013 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/blind-tech/</link>
        <guid isPermaLink="true">http://localhost:4000/blind-tech/</guid>
        
        
        <category>profession</category>
        
        <category>research</category>
        
      </item>
    
  </channel>
</rss>
